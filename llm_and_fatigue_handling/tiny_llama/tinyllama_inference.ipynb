{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "from simple_faiss_vd import runtime_add, retrieve_similar_vectors\n",
    "\n",
    "# === Authenticate Hugging Face ===\n",
    "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "if HUGGINGFACE_TOKEN is None:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN environment variable not set.\")\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# === Config ===\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MODEL_DIR = \"./tinyllama_prefix_final_model\"\n",
    "MAX_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, token=HUGGINGFACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# === Quantization Config ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# === Load Base Model ===\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# === Fix: Resize embeddings to match tokenizer size\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# === Load LoRA adapter ===\n",
    "llama_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    MODEL_DIR,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=False\n",
    ")\n",
    "llama_model.eval()\n",
    "\n",
    "# === Input Features and Fatigue Levels ===\n",
    "features = [24, 8, 0.38, 0.23, 96.0, 0.4, 0.21, 8.0, 1.3]\n",
    "fatigue_levels = [\"low\", \"medium\", \"medium\"]\n",
    "\n",
    "# === FAISS Retrieval (Optional) ===\n",
    "feature_vector = np.array(features, dtype=np.float32)\n",
    "results = retrieve_similar_vectors(feature_vector, k=3)\n",
    "retrieved_interventions = [\n",
    "    meta.get(\"intervention\") for _, meta, _ in results\n",
    "    if meta.get(\"intervention\") and meta.get(\"intervention\").strip().lower() not in {\"\", \"none\", \"driver alert\"}\n",
    "]\n",
    "\n",
    "context = \"\"\n",
    "if retrieved_interventions:\n",
    "    context = \"Previously suggested interventions for similar scenarios: \" + \"; \".join(retrieved_interventions) + \". \"\n",
    "\n",
    "# === Build Prompt ===\n",
    "prompt = f\"\"\"\n",
    "You are an intelligent in-cabin assistant.\n",
    "\n",
    "Fatigue levels:\n",
    "- Camera: {fatigue_levels[0]}\n",
    "- Steering: {fatigue_levels[1]}\n",
    "- Lane: {fatigue_levels[2]}\n",
    "\n",
    "Driving behavior features:\n",
    "- Blink rate: {features[0]:.1f} per minute\n",
    "- Yawning rate: {features[1]:.1f} per minute\n",
    "- PERCLOS: {features[2]:.2f}%\n",
    "- SDLP: {features[3]:.2f} m\n",
    "- Lane keeping ratio: {features[4]:.1f}\n",
    "- Lane departure frequency: {features[5]:.1f} per minute\n",
    "- Steering entropy: {features[6]:.2f}\n",
    "- Steering reversal rate: {features[7]:.1f} per minute\n",
    "- Steering angle variability: {features[8]:.2f}°\n",
    "\n",
    "{context}Based on the above driver state and past examples, suggest **only one** intervention to keep the driver alert.\n",
    "\n",
    "⚠️ IMPORTANT: You must output in this fixed format — no extra text, no repetition.\n",
    "\n",
    "Fan: Level X      ← X is a number like 1, 2, or 3  \n",
    "Music: On/Off  \n",
    "Vibration: On/Off  \n",
    "Reason: <short explanation of the logic>\n",
    "\n",
    "Example output:\n",
    "Fan: Level 2  \n",
    "Music: On  \n",
    "Vibration: Off  \n",
    "Reason: High blink rate and PERCLOS indicate moderate drowsiness.\n",
    "\n",
    "Now, provide your single final intervention and stop.\n",
    "\"\"\".strip()\n",
    "\n",
    "# === Tokenize ===\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "# === Generate ===\n",
    "with torch.no_grad():\n",
    "    output = llama_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# === Decode\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# === Postprocess: Extract only the first 'Fan:' block\n",
    "if response.count(\"Fan:\") > 1:\n",
    "    first_block = response.split(\"Fan:\")[1]\n",
    "    if \"Reason:\" in first_block:\n",
    "        reason_part = first_block.split(\"Reason:\")\n",
    "        first_block = reason_part[0] + \"Reason:\" + reason_part[1].split(\"\\n\")[0]\n",
    "    final_response = \"Fan:\" + first_block.strip()\n",
    "else:\n",
    "    intervention_start = response.find(\"Fan:\")\n",
    "    final_response = response[intervention_start:].strip() if intervention_start != -1 else response.strip()\n",
    "\n",
    "# === Output\n",
    "print(\"\\n=== Generated Intervention ===\")\n",
    "print(final_response)\n",
    "\n",
    "# === Save to vector DB\n",
    "runtime_add(feature_vector, intervention=final_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
