{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from model_wrapper_with_mlp_adapter import FeaturePrefixAdapter\n",
    "from faiss_vd import runtime_add, retrieve_similar_vectors\n",
    "import os\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_ID = \"meta-llama/Llama-2-7b-hf\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive/llm/LLM-based-Agent-for-Driver-Sleepiness-Detection-and-Mitigation-in-Automotive-Systems/llm_and_fatigue_handling/llama_prefix_final_model\"\n",
    "FEATURE_DIM = 9\n",
    "EMBEDDING_DIM = 4096\n",
    "PREFIX_TOKEN_COUNT = 5\n",
    "MAX_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Quant config for 4-bit loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# === Load tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === Load base model ===\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "\n",
    "# === Load LoRA adapter ===\n",
    "llama_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    MODEL_DIR,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "llama_model.eval()\n",
    "\n",
    "# === Load MLP Adapter ===\n",
    "adapter = FeaturePrefixAdapter(\n",
    "    input_dim=FEATURE_DIM,\n",
    "    hidden_dim=256,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    num_tokens=PREFIX_TOKEN_COUNT\n",
    ")\n",
    "adapter.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"prefix_adapter.pth\")))\n",
    "target_dtype = next(llama_model.parameters()).dtype\n",
    "adapter = adapter.to(dtype=target_dtype, device=DEVICE)\n",
    "adapter.eval()\n",
    "\n",
    "# === Input Features and Fatigue Levels ===\n",
    "features = [24, 8, 0.38, 0.23, 96.0, 0.4, 0.21, 8.0, 1.3]\n",
    "fatigue_levels = [\"low\", \"medium\", \"medium\"]\n",
    "\n",
    "# === Get prefix embedding\n",
    "feature_tensor = torch.tensor([features], dtype=target_dtype).to(DEVICE)\n",
    "prefix_embeddings = adapter(feature_tensor)  # [1, 5, 4096]\n",
    "\n",
    "# === Prepare FAISS vector\n",
    "token_matrix = prefix_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "# === Retrieve top-k similar interventions\n",
    "results = retrieve_similar_vectors(token_matrix, k=3)\n",
    "retrieved_interventions = [\n",
    "    meta.get(\"intervention\") for _, meta, _ in results\n",
    "    if meta.get(\"intervention\") and meta.get(\"intervention\").strip().lower() not in {\"\", \"none\", \"driver alert\"}\n",
    "]\n",
    "\n",
    "# === RAG-style context\n",
    "if retrieved_interventions:\n",
    "    context = \"Previously suggested interventions for similar scenarios: \" + \"; \".join(retrieved_interventions) + \". \"\n",
    "else:\n",
    "    context = \"\"\n",
    "\n",
    "# === Prompt with context + fatigue levels\n",
    "prompt = f\"\"\"\n",
    "{context}\n",
    "You are an intelligent in-cabin assistant.\n",
    "\n",
    "Fatigue levels:\n",
    "- Camera: {fatigue_levels[0]}\n",
    "- Steering: {fatigue_levels[1]}\n",
    "- Lane: {fatigue_levels[2]}\n",
    "\n",
    "Based on the above driver state and past examples, suggest an intervention to keep the driver alert.\n",
    "\n",
    "⚠️ IMPORTANT: You must output in this fixed format — no extra text.\n",
    "\n",
    "Fan: Level X      ← X is a number like 1, 2, or 3  \n",
    "Music: On/Off  \n",
    "Vibration: On/Off  \n",
    "Reason: <short explanation of the logic>\n",
    "\n",
    "Example output:\n",
    "Fan: Level 2  \n",
    "Music: On  \n",
    "Vibration: Off  \n",
    "Reason: High blink rate and PERCLOS indicate moderate drowsiness.\n",
    "\n",
    "Now, provide your intervention:\n",
    "\"\"\".strip()\n",
    "\n",
    "# === Tokenize prompt\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGTH - PREFIX_TOKEN_COUNT,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "# === Embeddings\n",
    "input_embeddings = llama_model.base_model.get_input_embeddings()(input_ids)\n",
    "prefix_embeddings = prefix_embeddings.to(dtype=target_dtype)\n",
    "input_embeddings = input_embeddings.to(dtype=target_dtype)\n",
    "combined_embeddings = torch.cat([prefix_embeddings, input_embeddings], dim=1)\n",
    "\n",
    "# === Attention Mask\n",
    "prefix_attention_mask = torch.ones(1, PREFIX_TOKEN_COUNT, dtype=torch.long).to(DEVICE)\n",
    "extended_attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "\n",
    "# === Generate\n",
    "with torch.no_grad():\n",
    "    output = llama_model.generate(\n",
    "        inputs_embeds=combined_embeddings,\n",
    "        attention_mask=extended_attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# === Decode\n",
    "response = tokenizer.decode(output[0, PREFIX_TOKEN_COUNT:], skip_special_tokens=True)\n",
    "print(\"\\n=== Generated Intervention ===\")\n",
    "print(response)\n",
    "\n",
    "# === Save final vector + output\n",
    "runtime_add(token_matrix, intervention=response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
