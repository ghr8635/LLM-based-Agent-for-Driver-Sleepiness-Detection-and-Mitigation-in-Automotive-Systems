{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from model_wrapper_with_mlp_adapter import FeaturePrefixAdapter\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_ID = \"meta-llama/Llama-2-7b-hf\"  # âœ… Smaller model to fit GPU\n",
    "MODEL_DIR = \"/content/drive/MyDrive/llm/LLM-based-Agent-for-Driver-Sleepiness-Detection-and-Mitigation-in-Automotive-Systems/llm_and_fatigue_handling/llama_prefix_final_model\"\n",
    "ADAPTER_PATH = os.path.join(MODEL_DIR, \"prefix_adapter.pth\")\n",
    "FEATURE_DIM = 9\n",
    "EMBEDDING_DIM = 4096\n",
    "PREFIX_TOKEN_COUNT = 5\n",
    "MAX_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Login to Hugging Face ===\n",
    "login(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "# === Fix fragmentation (optional, helpful for lower memory GPUs) ===\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# === BitsAndBytes Quantization config ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# === Load tokenizer and base model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix for missing pad_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "\n",
    "print(f\"Base model device: {next(base_model.parameters()).device}\")\n",
    "\n",
    "# === Load PEFT Adapter ===\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "llama_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "adapter_state_dict = torch.load(ADAPTER_PATH, map_location=\"cpu\")\n",
    "adapter_state_dict = {k.replace('base_model.', ''): v for k, v in adapter_state_dict.items()}\n",
    "llama_model.load_state_dict(adapter_state_dict, strict=False)\n",
    "\n",
    "print(f\"Peft model device: {next(llama_model.parameters()).device}\")\n",
    "\n",
    "# === Load MLP Feature Adapter ===\n",
    "adapter = FeaturePrefixAdapter(\n",
    "    input_dim=FEATURE_DIM,\n",
    "    hidden_dim=256,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    num_tokens=PREFIX_TOKEN_COUNT\n",
    ")\n",
    "adapter.load_state_dict(torch.load(ADAPTER_PATH, map_location=\"cpu\"))\n",
    "adapter = adapter.to(dtype=target_dtype, device=DEVICE)\n",
    "adapter.eval()\n",
    "\n",
    "print(f\"Adapter device: {next(adapter.parameters()).device}\")\n",
    "\n",
    "# === Sensor Features ===\n",
    "target_dtype = next(llama_model.parameters()).dtype  # Match model dtype\n",
    "features = [24, 8, 0.38, 0.23, 96.0, 0.4, 0.21, 8.0, 1.3]\n",
    "feature_tensor = torch.tensor([features], dtype=target_dtype).to(DEVICE) # Cast feature_tensor to target_dtype\n",
    "\n",
    "# === Tokenize Prompt ===\n",
    "short_prompt = \"Based on the above signals, what should be the appropriate intervention?\"\n",
    "inputs = tokenizer(\n",
    "    short_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGTH - PREFIX_TOKEN_COUNT,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "attention_mask = inputs[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "# === Generate Prefix Embeddings ===\n",
    "prefix_embeddings = adapter(feature_tensor)\n",
    "\n",
    "# === Get Input Embeddings from model ===\n",
    "embedding_layer = base_model.get_input_embeddings().to(DEVICE)\n",
    "input_embeddings = embedding_layer(input_ids)\n",
    "\n",
    "# Match dtypes (already done for feature_tensor, keeping this for input_embeddings)\n",
    "prefix_embeddings = prefix_embeddings.to(dtype=target_dtype)\n",
    "input_embeddings = input_embeddings.to(dtype=target_dtype)\n",
    "\n",
    "# === Combine Prefix + Input Embeddings ===\n",
    "combined_embeddings = torch.cat([prefix_embeddings, input_embeddings], dim=1)\n",
    "\n",
    "# === Extended Attention Mask ===\n",
    "prefix_attention_mask = torch.ones(1, PREFIX_TOKEN_COUNT, dtype=torch.long).to(DEVICE)\n",
    "extended_attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "\n",
    "# === Generate Output ===\n",
    "with torch.no_grad():\n",
    "    outputs = llama_model.generate(\n",
    "        inputs_embeds=combined_embeddings,\n",
    "        attention_mask=extended_attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# === Decode Output ===\n",
    "response = tokenizer.decode(outputs[0, PREFIX_TOKEN_COUNT:], skip_special_tokens=True)\n",
    "print(\"\\n=== Generated Intervention ===\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
