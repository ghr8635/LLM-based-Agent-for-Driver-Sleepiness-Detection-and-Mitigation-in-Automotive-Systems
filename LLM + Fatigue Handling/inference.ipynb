{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d5170",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from model_wrapper_with_mlp_adapter import FeaturePrefixAdapter, PrefixLLaMAModel\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "MODEL_PATH = \"/content/drive/MyDrive/llama2_driver_intervention_model_2\"\n",
    "PREFIX_TOKEN_COUNT = 5\n",
    "FEATURE_DIM = 9\n",
    "EMBEDDING_DIM = 4096\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Load tokenizer and model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "llama = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n",
    "adapter = FeaturePrefixAdapter(\n",
    "    input_dim=FEATURE_DIM,\n",
    "    hidden_dim=256,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    num_tokens=PREFIX_TOKEN_COUNT\n",
    ")\n",
    "adapter.load_state_dict(torch.load(os.path.join(MODEL_PATH, \"prefix_adapter.pth\"), map_location=DEVICE))\n",
    "\n",
    "# === Wrap the model\n",
    "model = PrefixLLaMAModel(llama, adapter).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === Minimal prompt\n",
    "MIN_PROMPT = \"Based on the above signals, what should be the appropriate intervention?\"\n",
    "\n",
    "# === Inference Function\n",
    "@torch.no_grad()\n",
    "def generate_intervention(features, max_new_tokens=100):\n",
    "    # Convert features to tensor and get prefix embedding\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    prefix_embed = model.adapter(features_tensor)  # shape: (1, PREFIX_TOKEN_COUNT, EMBEDDING_DIM)\n",
    "\n",
    "    # Tokenize the minimal prompt (no long context!)\n",
    "    inputs = tokenizer(MIN_PROMPT, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Get embedding for short prompt\n",
    "    text_embed = model.embedding_layer(input_ids)\n",
    "\n",
    "    # Concatenate: prefix + prompt embedding\n",
    "    full_embed = torch.cat([prefix_embed.to(text_embed.dtype), text_embed], dim=1)\n",
    "\n",
    "    # Update attention mask\n",
    "    prefix_mask = torch.ones((1, PREFIX_TOKEN_COUNT), dtype=attention_mask.dtype).to(DEVICE)\n",
    "    full_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "\n",
    "    # Generate output\n",
    "    output = model.llama.generate(\n",
    "        inputs_embeds=full_embed,\n",
    "        attention_mask=full_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# === Example\n",
    "if __name__ == \"__main__\":\n",
    "    example_features = [14.8, 3.2, 65.4, 0.28, 0.81, 1.9, 1.1, 3.4, 3.9]  # Replace with real-time input\n",
    "    result = generate_intervention(example_features)\n",
    "    print(\"Generated Intervention:\\n\", result)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
